{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43acb705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\coolp\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\coolp\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\coolp\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests in c:\\users\\coolp\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\coolp\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\coolp\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\coolp\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\coolp\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "List all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "!pip install bs4\n",
    "!pip install requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10222517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n",
      "ERROR: unknown command \"requests\"\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m terms \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Iterate over each row in the table\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     25\u001b[0m   \u001b[38;5;66;03m# Extract the name and term of office from the columns\u001b[39;00m\n\u001b[0;32m     26\u001b[0m   columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m   name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "#from https://presidentofindia.nic.in/former-presidents.htm and make data frame. \n",
    "!pip install\n",
    "!pip requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the table containing the information\n",
    "table = soup.find(\"table\")\n",
    "\n",
    "# Create empty lists to store the data\n",
    "names = []\n",
    "terms = []\n",
    "\n",
    "# Iterate over each row in the table\n",
    "for row in table.find_all(\"td\")[1:]:\n",
    "  # Extract the name and term of office from the columns\n",
    "  columns = row.find_all(\"td\")\n",
    "  name = columns[0].text.strip()\n",
    "  term = columns[1].text.strip()\n",
    "  \n",
    "  # Append the data to the respective lists\n",
    "  names.append(name)\n",
    "  terms.append(term)\n",
    "\n",
    "# Create a data frame using the lists\n",
    "data = {\"Name\": names, \"Term of Office\": terms}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the data frame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9766ed0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n",
      "ERROR: unknown command \"requests\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0        India\\nIND      55  6,640    121\n",
      "1    Australia\\nAUS      42  4,926    117\n",
      "2  South Africa\\nSA      34  3,750    110\n",
      "3     Pakistan\\nPAK      36  3,922    109\n",
      "4   New Zealand\\nNZ      43  4,399    102\n",
      "5      England\\nENG      38  3,777     99\n",
      "6     Sri Lanka\\nSL      47  4,134     88\n",
      "7   Bangladesh\\nBAN      44  3,836     87\n",
      "8  Afghanistan\\nAFG      30  2,533     84\n",
      "9   West Indies\\nWI      38  2,582     68\n",
      "                 Batsman Team Rating\n",
      "0           Shubman Gill  IND    826\n",
      "1             Babar Azam  PAK    824\n",
      "2            Virat Kohli  IND    791\n",
      "3           Rohit Sharma  IND    769\n",
      "4        Quinton de Kock   SA    760\n",
      "5         Daryl Mitchell   NZ    750\n",
      "6           David Warner  AUS    745\n",
      "7  Rassie van der Dussen   SA    735\n",
      "8           Harry Tector  IRE    729\n",
      "9            Dawid Malan  ENG    729\n",
      "           Bowler Team Rating\n",
      "0  Keshav Maharaj   SA    741\n",
      "1  Josh Hazlewood  AUS    703\n",
      "2  Mohammed Siraj  IND    699\n",
      "3  Jasprit Bumrah  IND    685\n",
      "4      Adam Zampa  AUS    675\n",
      "5     Rashid Khan  AFG    667\n",
      "6   Kuldeep Yadav  IND    667\n",
      "7     Trent Boult   NZ    663\n",
      "8  Shaheen Afridi  PAK    650\n",
      "9  Mohammad Shami  IND    648\n"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "#Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "!pip install\n",
    "!pip requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)\n",
    "#Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsman = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)\n",
    "#Top 10 ODI bowlers along with the records of their team andrating.\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  bowler = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  bowler_data.append([bowler, team, rating])\n",
    "\n",
    "df = pd.DataFrame(bowler_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff40d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n",
      "ERROR: unknown command \"requests\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,429    163\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      20  1,768     88\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      14  1,074     77\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      24  1,602     67\n",
      "                Batsman Team Rating\n",
      "0  Natalie Sciver-Brunt  ENG    807\n",
      "1           Beth Mooney  AUS    750\n",
      "2   Chamari Athapaththu   SL    736\n",
      "3       Laura Wolvaardt   SA    727\n",
      "4       Smriti Mandhana  IND    708\n",
      "5          Alyssa Healy  AUS    698\n",
      "6          Ellyse Perry  AUS    697\n",
      "7      Harmanpreet Kaur  IND    694\n",
      "8           Meg Lanning  AUS    662\n",
      "9        Marizanne Kapp   SA    642\n",
      "              Bowler Team Rating\n",
      "0  Sophie Ecclestone  ENG    746\n",
      "1     Shabnim Ismail   SA    680\n",
      "2      Jess Jonassen  AUS    662\n",
      "3       Megan Schutt  AUS    658\n",
      "4   Ashleigh Gardner  AUS    652\n",
      "5     Ayabonga Khaka   SA    640\n",
      "6         Kate Cross  ENG    628\n",
      "7    Hayley Matthews   WI    625\n",
      "8      Deepti Sharma  IND    607\n",
      "9     Marizanne Kapp   SA    600\n"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "# ) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "!pip install\n",
    "!pip requests\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "team_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  team = cells[1].text.strip()\n",
    "  matches = cells[2].text.strip()\n",
    "  points = cells[3].text.strip()\n",
    "  rating = cells[4].text.strip()\n",
    "  team_data.append([team, matches, points, rating])\n",
    "\n",
    "df = pd.DataFrame(team_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "print(df)\n",
    "#op 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "batsman_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  batsman = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  batsman_data.append([batsman, team, rating])\n",
    "\n",
    "df = pd.DataFrame(batsman_data, columns=[\"Batsman\", \"Team\", \"Rating\"])\n",
    "print(df)\n",
    "# Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "bowler_data = []\n",
    "table = soup.find(\"table\", class_=\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "\n",
    "for row in rows[1:11]:\n",
    "  cells = row.find_all(\"td\")\n",
    "  bowler = cells[1].text.strip()\n",
    "  team = cells[2].text.strip()\n",
    "  rating = cells[3].text.strip()\n",
    "  bowler_data.append([bowler, team, rating])\n",
    "\n",
    "df = pd.DataFrame(bowler_data, columns=[\"Bowler\", \"Team\", \"Rating\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a07032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n",
      "ERROR: unknown command \"requests\"\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m headlines\u001b[38;5;241m.\u001b[39mappend(headline)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Extract the time\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[43marticle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     35\u001b[0m times\u001b[38;5;241m.\u001b[39mappend(time)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Extract the news link\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "#make data frame-\n",
    "# Headline\n",
    "#Time\n",
    "#News Linkimport requests\n",
    "!pip install \n",
    "!pip requests\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the news articles on the page\n",
    "articles = soup.find_all(\"div\", class_=\"Card-titleContainer\")\n",
    "\n",
    "# Initialize empty lists to store the scraped data\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "# Loop through each article and extract the required information\n",
    "for article in articles:\n",
    "  # Extract the headline\n",
    "  headline = article.find(\"a\").text.strip()\n",
    "  headlines.append(headline)\n",
    "  \n",
    "  # Extract the time\n",
    "  time = article.find(\"time\").text.strip()\n",
    "  times.append(time)\n",
    "  \n",
    "  # Extract the news link\n",
    "  link = article.find(\"a\")[\"href\"]\n",
    "  links.append(link)\n",
    "\n",
    "# Create a dataframe using the scraped data\n",
    "data = {\n",
    "  \"Headline\": headlines,\n",
    "  \"Time\": times,\n",
    "  \"News Link\": links\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f06e3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n",
      "ERROR: unknown command \"requests\"\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m urls \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Iterate over each article in the container\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m \u001b[43marticles_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     34\u001b[0m   \u001b[38;5;66;03m# Scrape the title\u001b[39;00m\n\u001b[0;32m     35\u001b[0m   title \u001b[38;5;241m=\u001b[39m article\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     36\u001b[0m   titles\u001b[38;5;241m.\u001b[39mappend(title)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "#days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "#Scrape below mentioned details and make data frame-\n",
    "# Paper Title\n",
    "# Authors\n",
    "# Published Date\n",
    "#Paper URL\n",
    "\n",
    "!pip install\n",
    "!pip requests \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the URL\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the container that holds the article details\n",
    "articles_container = soup.find(\"div\", class_=\"pod-listing\")\n",
    "\n",
    "# Initialize empty lists to store the scraped data\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "# Iterate over each article in the container\n",
    "for article in articles_container.find_all(\"li\"):\n",
    "  # Scrape the title\n",
    "  title = article.find(\"h3\").text.strip()\n",
    "  titles.append(title)\n",
    "  \n",
    "  # Scrape the authors\n",
    "  author = article.find(\"span\", class_=\"text-xs\").text.strip()\n",
    "  authors.append(author)\n",
    "  \n",
    "  # Scrape the published date\n",
    "  date = article.find(\"span\", class_=\"text-xs\").find_next_sibling(\"span\").text.strip()\n",
    "  dates.append(date)\n",
    "  \n",
    "  # Scrape the paper URL\n",
    "  url = article.find(\"a\")[\"href\"]\n",
    "  urls.append(url)\n",
    "\n",
    "# Create a dataframe with the scraped data\n",
    "data = {\n",
    "  \"Paper Title\": titles,\n",
    "  \"Authors\": authors,\n",
    "  \"Published Date\": dates,\n",
    "  \"Paper URL\": urls\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d55b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
